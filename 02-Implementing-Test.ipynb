{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Shopee Code League 2020\n",
    "\n",
    "## 2.0 Implementing Trained Model\n",
    "\n",
    "### Introduction\n",
    "After reviewing the task from [Shopee Code League 2020 - Sentiment Analysis](https://www.kaggle.com/c/student-shopee-code-league-sentiment-analysis/overview), our team has built and trained the earlier model. \n",
    "\n",
    "We will now be using this script to implement the model before submission to kraggle to obtain test score before further fine-tuning our script to improve scores.\n",
    "\n",
    "To do this, we will prepare the test data, unPickle and test with the trained model before saving into the output file.\n",
    "\n",
    "#### Team Introduction\n",
    "Team Name: **JNNY** <br/>\n",
    "Team Members: **Natalie, James, Yong Xian, Nicky** <br/>\n",
    "Script Prepared by: **Nicky** [@ahjimomo](https://github.com/ahjimomo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Library & Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great danger, cool, motif and cantik2 jg model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>One of the shades don't fit well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Very comfortable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Fast delivery. Product expiry is on Dec 2022. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>it's sooooo cute! i like playing with the glit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60422</th>\n",
       "      <td>60423</td>\n",
       "      <td>Product has been succesfully ordered and shipp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60423</th>\n",
       "      <td>60424</td>\n",
       "      <td>Opening time a little scared. Fear dalemnya de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60424</th>\n",
       "      <td>60425</td>\n",
       "      <td>The product quality is excellent. The origina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60425</th>\n",
       "      <td>60426</td>\n",
       "      <td>They 're holding up REALLY well also .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60426</th>\n",
       "      <td>60427</td>\n",
       "      <td>Rapid response and detail ...\\nThanks gan, the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60427 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_id                                             review\n",
       "0              1  Great danger, cool, motif and cantik2 jg model...\n",
       "1              2                   One of the shades don't fit well\n",
       "2              3                                   Very comfortable\n",
       "3              4  Fast delivery. Product expiry is on Dec 2022. ...\n",
       "4              5  it's sooooo cute! i like playing with the glit...\n",
       "...          ...                                                ...\n",
       "60422      60423  Product has been succesfully ordered and shipp...\n",
       "60423      60424  Opening time a little scared. Fear dalemnya de...\n",
       "60424      60425   The product quality is excellent. The origina...\n",
       "60425      60426             They 're holding up REALLY well also .\n",
       "60426      60427  Rapid response and detail ...\\nThanks gan, the...\n",
       "\n",
       "[60427 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "test_raw = pd.read_csv('input/test.csv')\n",
    "\n",
    "test_raw # 60427 reviews imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Cleaning & Preparation\n",
    "\n",
    "Similarly to how we prepare the date for training of the model, we will prepare the data for the test model as well.\n",
    "\n",
    "The basic text pre-processing techniques will be similar as per the train set with [Python Regular Expressions](https://developers.google.com/edu/python/regular-expressions) as follows <br/>\n",
    "    - Removing regular expressions\n",
    "    - Removing single characters\n",
    "    - Removing words containing numbers\n",
    "    - Removing multiple whitespaces\n",
    "    - Making text all lowercase\n",
    "\n",
    "After the cleaning process, we will then move to tokenizing the data for the TF-IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great danger, cool, motif and cantik2 jg models. Delivery cepet. Tp packing less okay krn only wear clear plastic nerawang klihtan contents jd \n",
      "\n",
      "One of the shades don't fit well \n",
      "\n",
      "Very comfortable \n",
      "\n",
      "Fast delivery. Product expiry is on Dec 2022. Product wrap properly. No damage on the item. \n",
      "\n",
      "it's sooooo cute! i like playing with the glitters better than browsing on my phone now. item was also deliered earlier than i expected. thank you seller! may you have more buyers to come. ðŸ˜ŠðŸ˜ŠðŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the labels & sentiments from the training data\n",
    "\n",
    "test_features = test_raw.iloc[:, 1].values\n",
    "\n",
    "# Check features\n",
    "i = 0\n",
    "\n",
    "while i < 5:\n",
    "    print(test_features[i], \"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_features = []\n",
    "\n",
    "for sentence in range(0, len(test_features)):\n",
    "    \n",
    "    # Remove all special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(test_features[sentence]))\n",
    "    \n",
    "    # Remove all words that include digits / numbers\n",
    "    processed_feature = re.sub(r'\\w*\\d\\w*', ' ', processed_feature)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    processed_feature = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    \n",
    "    # Substituing multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags = re.I)\n",
    "    \n",
    "    # Converting to lowercase\n",
    "    processed_featured = processed_feature.lower()\n",
    "    \n",
    "    # Append cleaned review to processed list\n",
    "    processed_test_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great danger cool motif and jg models Delivery cepet Tp packing less okay krn only wear clear plastic nerawang klihtan contents jd \n",
      "\n",
      "One of the shades don fit well \n",
      "\n",
      "Very comfortable \n",
      "\n",
      "Fast delivery Product expiry is on Dec Product wrap properly No damage on the item  \n",
      "\n",
      "it sooooo cute like playing with the glitters better than browsing on my phone now item was also deliered earlier than expected thank you seller may you have more buyers to come  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking if labels / reviews have been cleaned up\n",
    "i = 0\n",
    "\n",
    "while i < 5:\n",
    "    print(processed_test_features[i], \"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "We will use the same `TF-IDF` vectorizer that would include the function of tokenization and filtering of stop words to help with our test dataset. <br/>\n",
    "\n",
    "Similarly, we will use `max_features` = 2500 with `max_df` of 80% to remove the all too common words but a revision to reduce the `min_df` of 5 as the test set in smaller in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1bf6780bd8a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprocessed_test_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_test_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\shopeeleague\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \"\"\"\n\u001b[0;32m   1839\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shopeeleague\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shopeeleague\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m             \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 2500, min_df = 5, max_df = 0.8, stop_words = stopwords.words('english'))\n",
    "processed_test_features = vectorizer.fit_transform(processed_test_features).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Time to predict the test data for submission! \n",
    "\n",
    "We will have to now the unPickle the trained model to make predictions to the test data & output in the format required by the challenge.\n",
    "\n",
    "Since we have 2 trained models, we will prepare 2 sets of submission in `csv` format. \n",
    "\n",
    "The task requires our submission to have only 2 columns:\n",
    "    - review_id (int) which are the indexes of the reviews, starting from 0\n",
    "    - rating (int) the predictions made by our model\n",
    "    - output in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnPickle our earlier trained model\n",
    "\n",
    "# Model 1 : Accuracy of 45.5% on y-test\n",
    "\n",
    "filename_01 = \"rfc_model_01.pkl\"\n",
    "with open(filename_01, 'rb') as file1:\n",
    "    pickle_rfc_01 = pickle.load(file1)\n",
    "    \n",
    "# Model 2 : Accuracy of 45.36% on y-test\n",
    "filename_02 = \"rfc_model_02.pkl\"\n",
    "with open(filename_02, 'rb') as file2:\n",
    "    pickle_rfc_02 = pickle.load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to predict!\n",
    "\n",
    "predictions_01 = pickle_rfc_01.predict(processed_test_features)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_02 = pickle_rfc_02.predict(processed_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the prediction output accurate & is between 1 to 5\n",
    "# Check length to ensure input features = output predicted labels\n",
    "\n",
    "print(min(predictions_01), min(predictions_02))\n",
    "print(max(predictions_01), max(predictions_02))\n",
    "print(len(predictions_01), len(predictions_02), \"\\nOriginal:\", len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prearing the submission document\n",
    "\n",
    "We will need to attach the predictions to the original indexing before removing the features (reviews/sentiments) to have the document in the correct submission format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copies for ease of reusibility later\n",
    "\n",
    "submission_01 = test_raw.copy()\n",
    "\n",
    "submission_02 = test_raw.copy()\n",
    "\n",
    "# Checking if copy is successful\n",
    "submission_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching the predictions to it's review_id\n",
    "submission_01['rating'] = predictions_01\n",
    "submission_02['rating'] = predictions_02\n",
    "\n",
    "# Checking if columns are correctly attached\n",
    "submission_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the review column\n",
    "\n",
    "submission_01 = submission_01.drop(\"review\", axis = 1)\n",
    "submission_02 = submission_02.drop(\"review\", axis = 1)\n",
    "\n",
    "# Checks if column has been dropped correctly\n",
    "submission_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last check before saving to csv.\n",
    "\n",
    "print(\"submission doc 1:\", submission_01.shape)\n",
    "print(\"submission doc 2:\", submission_02.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results in csv format\n",
    "\n",
    "submission_01.to_csv(\"shopeecodeleague_SentimentAnalysis_TeamJnny_01.csv\", index = False, encoding = 'utf8')\n",
    "submission_02.to_csv(\"shopeecodeleague_SentimentAnalysis_TeamJnny_02.csv\", index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "We have now completed & submitted our work. The test score as follows on the [Kraggle Leaderboard](https://www.kaggle.com/c/student-shopee-code-league-sentiment-analysis/leaderboard#score):\n",
    "\n",
    "`Model_01`: 0.018133 <br/>\n",
    "`Model_02`: 0.018122\n",
    "\n",
    "Seems really poor in performance, how can we improve it? In `part 3`, we shall look into adding other techniques in hopes to improve our model and the result.\n",
    "\n",
    "#### Thank you ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
